{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36eeb0f4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Импорты-и-создание-сессии\" data-toc-modified-id=\"Импорты-и-создание-сессии-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Импорты и создание сессии</a></span></li><li><span><a href=\"#Изучение-данных\" data-toc-modified-id=\"Изучение-данных-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Изучение данных</a></span></li><li><span><a href=\"#DecisionTreeRegressor\" data-toc-modified-id=\"DecisionTreeRegressor-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DecisionTreeRegressor</a></span></li><li><span><a href=\"#RandomForestRegressor\" data-toc-modified-id=\"RandomForestRegressor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>RandomForestRegressor</a></span></li><li><span><a href=\"#GBTRegressor\" data-toc-modified-id=\"GBTRegressor-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>GBTRegressor</a></span></li><li><span><a href=\"#Подготовка-snippets\" data-toc-modified-id=\"Подготовка-snippets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Подготовка snippets</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ea53d",
   "metadata": {},
   "source": [
    "<footer id=\"footer\"></footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141ea20",
   "metadata": {},
   "source": [
    "<p align='center'>Ml Engeenering</p>\n",
    "<p align=\"center\"><img src=\"https://drive.google.com/uc?id=1X5HPpSb2Bk2QRXZzZy-Xp_vfwMyKF8ly\" width=500 border=\"0\"></a></p>\n",
    "\n",
    "\n",
    "\n",
    "Нам необходимо провести исследование, выбрать лучший тип модели и реализовать распределенную модель, для этого, вы можете задействовать все модели регрессии (`DecisionTreeRegressor`, `RandomForestRegressor`, `GBTRegressor`), подобрать оптимальные гиперпараметры, сравнить результаты и выбрать лучшую для дальнейшего применения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee97b5e",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2882bd5",
   "metadata": {},
   "source": [
    "## Импорты и создание сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f3ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import (DecisionTreeRegressor,\n",
    "                                       RandomForestRegressor,\n",
    "                                       GBTRegressor)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import (ParamGridBuilder, \n",
    "                               TrainValidationSplit)\n",
    "SEED=42\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e539d5a",
   "metadata": {},
   "source": [
    "Создадим спарк-сессию и приступим к работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/05 21:24:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML_project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d57d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://artykrafty:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ML_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10fba08e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf40d90",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f4aa2",
   "metadata": {},
   "source": [
    "## Изучение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52876197",
   "metadata": {},
   "source": [
    "Загрузим наши выборки и посмотрим первые строки. Поймем, что за тип данных у нас в работе и какие потребуетются преобразование в дальнейшем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b788b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = spark.read.parquet(\"train.parquet\", header=True)\n",
    "test = spark.read.parquet(\"test.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd53b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "|ad_id|target_audience_count|has_video|is_cpm|is_cpc|         ad_cost|day_count|              ctr|\n",
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "|    1|     10707.2440058622|        1|     1|     0|201.829292651124|       15|0.431740082807281|\n",
      "|    5|     10643.3872649482|        1|     1|     0|192.577221699704|       15|0.809264519216201|\n",
      "|    6|     11418.7085911347|        1|     1|     0|204.104562956739|       11|0.909738306804039|\n",
      "|    7|     10109.3278687796|        1|     1|     0|194.255798599684|       12|0.941221039774456|\n",
      "|    8|     10665.1119991977|        1|     1|     0|202.658042557742|       14|0.986790019690954|\n",
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd2079",
   "metadata": {},
   "source": [
    "Структура данных, которую мы будем использовать:\n",
    "\n",
    "| Имя признака  | Тип признака   | Описание  |\n",
    "|:---:|:---:|:---:|\n",
    "|  ad_id |  integer |  id рекламного объявления | \n",
    "|  target_audience_count\t | decimal  | размер аудитории, на которую таргетируется объявление  |\n",
    "| has_video  | integer  |  1 если есть видео, иначе 0 |\n",
    "|  is_cpm |  integer |\t1 если тип объявления CPM, иначе 0   | \n",
    "|  is_cpc |   integer|   1 если тип объявления CPC, иначе 0| \n",
    "|  ad_cost |   double|  стоимость объявления в рублях | \n",
    "|  day_count |  integer |  Число дней, которое показывалась реклама | \n",
    "|  ctr |  double |  Отношение числа кликов к числу просмотров | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99a0b",
   "metadata": {},
   "source": [
    "Типы данных нас устраивают. Первое что нам необходимо сделать - привести данные к векторному виду, с которым работает `sparkML`. Делить выборку на валидацию и обучение не будем - сделаем это в процессе подбора параметров. Можно исключить признак `ad_id` из вектора. Также добавим стандартизацию для нашего вектора\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc9f10",
   "metadata": {},
   "source": [
    "Теперь приступим к построению пайплайна. Определим шаги:\n",
    "\n",
    "\n",
    "1. Нам необходимо преобразовать в вектор наши фреймы. \n",
    "2. Создать модель. \n",
    "3. Получить предсказания\n",
    "\n",
    "Но перед этим, мы попробуем модели на тесте и проведем подбор гиперпараметров - поэтому наш пайплайн будет пока включать только шаг `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec9c61",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe96e4",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bb7b6",
   "metadata": {},
   "source": [
    "Составим пайплайн и попробуем подобрать параметры, затем и получим предсказания на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58c2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = VectorAssembler(inputCols=train.columns[1:-1], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=False)\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol=\"ctr\", metricName=\"rmse\")\n",
    "\n",
    "dt_model = DecisionTreeRegressor(labelCol=\"ctr\", featuresCol=\"scaled_features\", seed=SEED)\n",
    "pipeline = Pipeline(stages=[vector, scaler, dt_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7488a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (ParamGridBuilder()\n",
    "          .addGrid(dt_model.maxDepth, [2, 3, 4, 5])\n",
    "          .addGrid(dt_model.minInfoGain, [0.1, 0.2, 0.4])\n",
    "          .build()\n",
    "         )\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=params,\n",
    "                          evaluator=evaluator,\n",
    "                          trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6c69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_tree = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4064e8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49793226083037,\n",
       " 0.49793226083037,\n",
       " 0.6964096428567128,\n",
       " 0.3156563505343809,\n",
       " 0.3156563505343809,\n",
       " 0.6964096428567128,\n",
       " 0.3156563505343809,\n",
       " 0.3156563505343809,\n",
       " 0.6964096428567128,\n",
       " 0.3156563505343809,\n",
       " 0.3156563505343809,\n",
       " 0.6964096428567128]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tree.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac01aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на тесте: 0.315\n"
     ]
    }
   ],
   "source": [
    "model_best = model_tree.bestModel\n",
    "pred_test = model_best.transform(test)\n",
    "# считаем rmse\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol=\"ctr\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(pred_test)\n",
    "\n",
    "\n",
    "print(\"RMSE на тесте:\", round(rmse, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efb757",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdd6f0",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf76a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(labelCol=\"ctr\", featuresCol=\"scaled_features\", seed=SEED)\n",
    "pipeline = Pipeline(stages=[vector, scaler, rf_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8628343",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (ParamGridBuilder()\n",
    "          .addGrid(rf_model.maxDepth, [2, 3, 4, 5])\n",
    "          .addGrid(rf_model.numTrees, [5, 10, 12])\n",
    "          .build()\n",
    "         )\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=params,\n",
    "                          evaluator=evaluator,\n",
    "                          trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11af8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_forest = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42d5d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на валидации: 0.319\n"
     ]
    }
   ],
   "source": [
    "# считаем rmse\n",
    "model_best = model_forest.bestModel\n",
    "preds = model_best.transform(test)\n",
    "# считаем rmse\n",
    "rmse = evaluator.evaluate(preds)\n",
    "\n",
    "print(\"RMSE на валидации:\", round(rmse, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995ca4b",
   "metadata": {},
   "source": [
    "На данной задаче дерево работает лучше, чем лес - попробуем модель с градиентным бустингом и тогда, выбрав лучшую - приступим к построению пайплайна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ae79e",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3861658",
   "metadata": {},
   "source": [
    "## GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c592a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model = GBTRegressor(labelCol=\"ctr\", featuresCol=\"scaled_features\", seed=SEED)\n",
    "pipeline = Pipeline(stages=[vector, scaler, gbt_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79aa2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (ParamGridBuilder()\n",
    "          .addGrid(gbt_model.maxDepth, [2, 3, 4, 5])\n",
    "          .addGrid(gbt_model.maxIter, [15, 20, 25])\n",
    "          .build()\n",
    "         )\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=params,\n",
    "                          evaluator=evaluator,\n",
    "                          trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2434d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/05 21:25:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/06/05 21:25:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_gbt = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb463c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на валидации: 0.256\n"
     ]
    }
   ],
   "source": [
    "# считаем rmse\n",
    "model_best = model_gbt.bestModel\n",
    "pred_test = model_best.transform(test)\n",
    "# считаем rmse\n",
    "rmse = evaluator.evaluate(pred_test)\n",
    "\n",
    "print(\"RMSE на валидации:\", round(rmse, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42f0bf",
   "metadata": {},
   "source": [
    "Таким образом - лучшая модель - модель на основе Градиентного бустинга - остановимся на ней. Сформируем итоговый пайплайн на базе этой модели и подготовим необходимые скрипты. Проверим, что все работает как надо. Если все нормально - сохраняем pipeline в виде модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0b41a",
   "metadata": {},
   "source": [
    "Все работает как задумано. Теперь мы можем переходить к подготовке скриптов. Напишем сниппеты, которые лягут в основу и затем соберем скрипты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a01723",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212a105",
   "metadata": {},
   "source": [
    "## Подготовка snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e0983",
   "metadata": {},
   "source": [
    "Нам необходимо подготовить два скрипта - один на обучение, подбор гиперапараметров и выбор лучшей модели, второй на получения предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ea9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import logging\n",
    "SEED = 42\n",
    "MODEL_PATH = 'spark_ml_model'\n",
    "\n",
    "\n",
    "log_classes = {\n",
    "    'init': 'INIT',\n",
    "    'metric': 'METRIC',\n",
    "    'model': 'MODEL',\n",
    "    'process': 'PROCESSING',\n",
    "    'complete': 'COMPLETED'\n",
    "}\n",
    "\n",
    "def process(spark:SparkSession, train_data: Path, test_data: Path) -> None:\n",
    "    \"\"\"основной скрипт по валидации и получению параметров на тестовой выборке\n",
    "       создает три модели, подбирает параметры и получает окончательные результаты на тестовой выборке\n",
    "       сохраняет лучшую модель\"\"\"\n",
    "    # читаем train и test\n",
    "    train = spark.read.parquet(train_data, header=True)\n",
    "    test = spark.read.parquet(test_data, header=True)\n",
    "    \n",
    "    _log(log_classes['init'], 'Train and test are loaded')\n",
    "    # создаем эвалуатора для подсчета RMSE\n",
    "    evaluator = RegressionEvaluator(\n",
    "        predictionCol='prediction', labelCol=\"ctr\", metricName=\"rmse\")\n",
    "    _log(log_classes['init'], 'Evaluator created. Metric is RMSE')\n",
    "    # добавляем необходимые трансформации\n",
    "    vector = VectorAssembler(\n",
    "        inputCols=train.columns[1:-1], outputCol=\"features\")\n",
    "    _log(log_classes['init'], f'Features for processing are {train.columns[1:-1]}')\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=False)\n",
    "    # получаем список моделей и параметров\n",
    "    models = get_models()\n",
    "    _log(log_classes['init'], 'Models are loaded')\n",
    "    metrics = []\n",
    "    best_models = {}\n",
    "    # перебираем ключи в словаре моделей и производим валидацию и сбор результатов\n",
    "    for key in models.keys():\n",
    "        model = models[key][\"model\"]\n",
    "        model_name = model.__class__.__name__\n",
    "        _log(log_classes['process'], f'Working with {model_name}')\n",
    "        params = models[key][\"params\"]\n",
    "        # создаем пайплайн и tvs\n",
    "        pipeline = Pipeline(stages=[vector, scaler, model])\n",
    "        tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                                   estimatorParamMaps=params,\n",
    "                                   evaluator=evaluator,\n",
    "                                   trainRatio=0.8)\n",
    "        _log(log_classes['process'], 'Starting parametrs tuning')\n",
    "        # собираем метрики\n",
    "        fit_model = tvs.fit(train)\n",
    "        best_model = fit_model.bestModel\n",
    "        best_metric = evaluator.evaluate(best_model.transform(test))\n",
    "        _log(log_classes['metric'], f'The best metric on test for model {model_name} is {best_metric}')\n",
    "        metrics.append(best_metric)\n",
    "        best_models[key] = best_model\n",
    "    # выбираем лучшую модель\n",
    "    choose_model = get_best(metrics, best_models)\n",
    "    _log(log_classes['model'], f'The best model is {model_name}')\n",
    "    _log(log_classes['metric'], f'The best metric on test is {min(metrics)}')\n",
    "    choose_model.write().overwrite().save(MODEL_PATH)\n",
    "    _log(log_classes['complete'], f'Model is saved. Path is {MODEL_PATH}')\n",
    "    spark.stop()\n",
    "    _log(log_classes['complete'], 'Spark session is stoped. }')\n",
    "    \n",
    "def get_best(metrics: List[float], best_model:PipelineModel) -> PipelineModel:\n",
    "    \"\"\"отбираем лучшую модель\"\"\"\n",
    "    idx = np.argmax(metrics)\n",
    "    keys_list = list(best_model)\n",
    "    key = keys_list[idx]\n",
    "    return best_model[key]\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"создаем модели и фиксируем их сетку параметров для валидации\"\"\"\n",
    "    dt = DecisionTreeRegressor(\n",
    "        labelCol='ctr', featuresCol=\"scaled_features\", seed=SEED)\n",
    "    rf = RandomForestRegressor(\n",
    "        labelCol='ctr', featuresCol=\"scaled_features\", seed=SEED)\n",
    "    gbt = GBTRegressor(labelCol='ctr', featuresCol=\"scaled_features\", seed=SEED)\n",
    "\n",
    "    models_info = {\n",
    "        'dt': {'model': dt,\n",
    "               'params': ParamGridBuilder()\n",
    "                 .addGrid(dt.maxDepth, [2, 3, 4, 5])\n",
    "                 .addGrid(dt.minInfoGain, [0.1, 0.2, 0.4])\n",
    "                 .build()},\n",
    "        'rf': {'model': rf,\n",
    "               'params': ParamGridBuilder()\n",
    "                 .addGrid(rf.maxDepth, [2, 3, 4, 5])\n",
    "                 .addGrid(rf.numTrees, [5, 10, 12])\n",
    "                 .build()},\n",
    "        'gbt': {'model': gbt,\n",
    "                'params': ParamGridBuilder()\n",
    "                  .addGrid(gbt.maxDepth, [2, 3, 4, 5])\n",
    "                  .addGrid(gbt.maxIter, [15, 20, 25])\n",
    "                  .build()}\n",
    "    }\n",
    "    return models_info\n",
    "\n",
    "def _log( cat: str, info: str) -> None:\n",
    "    \"\"\"дял логирования результатов\"\"\"\n",
    "    record = f'{datetime.now()} {cat} {info}'\n",
    "    print(record)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f6b731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:27:09.807132 INIT Train and test are loaded\n",
      "2022-06-05 21:27:09.811926 INIT Evaluator created. Metric is RMSE\n",
      "2022-06-05 21:27:09.816217 INIT Features for processing are ['target_audience_count', 'has_video', 'is_cpm', 'is_cpc', 'ad_cost', 'day_count']\n",
      "2022-06-05 21:27:09.826906 INIT Models are loaded\n",
      "2022-06-05 21:27:09.826918 PROCESSING Working with DecisionTreeRegressor\n",
      "2022-06-05 21:27:09.827046 PROCESSING Starting parametrs tuning\n",
      "2022-06-05 21:27:29.870218 METRIC The best metric on test for model DecisionTreeRegressor is 0.31487318624194816\n",
      "2022-06-05 21:27:29.870307 PROCESSING Working with RandomForestRegressor\n",
      "2022-06-05 21:27:29.870613 PROCESSING Starting parametrs tuning\n",
      "2022-06-05 21:27:56.511255 METRIC The best metric on test for model RandomForestRegressor is 0.3186750437140377\n",
      "2022-06-05 21:27:56.511333 PROCESSING Working with GBTRegressor\n",
      "2022-06-05 21:27:56.511535 PROCESSING Starting parametrs tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3951:>                                                       (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:29:26.053088 METRIC The best metric on test for model GBTRegressor is 0.25587559375275926\n",
      "2022-06-05 21:29:26.053211 MODEL The best model is GBTRegressor\n",
      "2022-06-05 21:29:26.053218 METRIC The best metric on test is 0.25587559375275926\n",
      "2022-06-05 21:29:29.855091 COMPLETED Model is saved. Path is spark_ml_model\n",
      "2022-06-05 21:29:30.140378 COMPLETED Spark session is stoped. }\n"
     ]
    }
   ],
   "source": [
    "process(spark, \"train.parquet\", \"test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617dce3",
   "metadata": {},
   "source": [
    "Здесь все работает. Подготовим второй скрипт - для получения предсказаний и сохранения результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f84254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "MODEL_PATH = 'spark_ml_model'\n",
    "\n",
    "log_classes = {\n",
    "    'init': 'INIT',\n",
    "    'metric': 'METRIC',\n",
    "    'model': 'MODEL',\n",
    "    'process': 'PROCESSING',\n",
    "    'complete': 'COMPLETED'\n",
    "}\n",
    "\n",
    "\n",
    "def process(spark: SparkSession, input_file: Path, output_file: Path):\n",
    "    # input_file - путь к файлу с данными для которых нужно предсказать ctr\n",
    "    # output_file - путь по которому нужно сохранить файл с результатами [ads_id, prediction]\n",
    "    inputs = spark.read.parquet(input_file)\n",
    "    _log(log_classes['init'], 'Assets are loaded')\n",
    "    # грузим модель\n",
    "    model = PipelineModel.load(MODEL_PATH)\n",
    "    _log(log_classes['process'], 'Getting predicions')\n",
    "    # прогоняем инпуты и получаем предсказания\n",
    "    outputs = model.transform(inputs)\n",
    "    # сохраняем в csv\n",
    "    interes = outputs.select('ad_id', 'prediction')\n",
    "    _log(log_classes['process'], 'Example for preds data')\n",
    "    interes.show(1)\n",
    "    # сжимаем до одной партиции\n",
    "    _log(log_classes['process'], 'Saving predicions')\n",
    "    interes.coalesce(1) \\\n",
    "           .write.format(\"com.databricks.spark.csv\") \\\n",
    "           .option(\"header\", \"true\") \\\n",
    "           .save(output_file) \\\n",
    "    # останавливаем сессию\n",
    "    spark.stop()\n",
    "    _log(log_classes['complete'], 'Spark session is stoped')\n",
    "\n",
    "\n",
    "def _log(cat: str, info: str) -> None:\n",
    "    \"\"\"для логирования результатов\"\"\"\n",
    "    record = f'{datetime.now()} {cat} {info}'\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b505f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 21:30:21.904164 INIT Assets are loaded\n",
      "2022-06-05 21:30:24.986072 PROCESSING Getting predicions\n",
      "2022-06-05 21:30:25.134014 PROCESSING Example for preds data\n",
      "+-----+------------------+\n",
      "|ad_id|        prediction|\n",
      "+-----+------------------+\n",
      "|    2|3.4507970810654127|\n",
      "+-----+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "2022-06-05 21:30:25.371469 PROCESSING Saving predicions\n",
      "2022-06-05 21:30:26.582905 COMPLETED Spark session is stoped\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML_project\").getOrCreate()\n",
    "process(spark, \"test.parquet\", \"outputs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4e191",
   "metadata": {},
   "source": [
    "Проверим, что все работает как задумано"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c8b17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ML_project\").getOrCreate()\n",
    "df = spark.read.csv(\"outputs.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f76eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|ad_id|        prediction|\n",
      "+-----+------------------+\n",
      "|    2|3.4507970810654127|\n",
      "|    3|3.3604116301025093|\n",
      "|    4|3.4629504926752888|\n",
      "|   10|3.4559802785927745|\n",
      "|   13|3.3604116301025093|\n",
      "+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fbee190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad_id', 'prediction']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4258c",
   "metadata": {},
   "source": [
    "Все отработало как необходимо - можно приступить к подготовке самих скриптов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1d240",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    <a href=\"#footer\"><img src='https://img.shields.io/badge/К содержанию-&#x21A9-blue'></a>\n",
    "</div>\n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "652px",
    "left": "160px",
    "top": "186px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
